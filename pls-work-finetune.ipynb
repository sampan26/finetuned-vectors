{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/cma1114/enhanced_hooking.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U torch transformers matplotlib pandas scikit-learn seaborn datasets\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from enhanced_hooking import get_blocks, clear_hooks, attach_activation_hooks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN='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'\n",
    "def load_model(model_path, device, center_weights=True):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, token=HF_TOKEN).to(device)\n",
    "    if center_weights:\n",
    "        for name, param in model.named_parameters():\n",
    "            if '.'.join(name.split('.')[-2:]) in ['wte.weight','wpe.weight','c_proj.weight','c_proj.bias']:\n",
    "                param.data -= param.data.mean()\n",
    "                print(name, param.data.mean(), param.size())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=HF_TOKEN)\n",
    "    model.tokenizer = tokenizer\n",
    "    model.tokenizer.padding_side = \"left\" #for batching; right (default in gpt2) for training, left for generation\n",
    "    model.tokenizer.pad_token_id = model.tokenizer.eos_token_id \n",
    "    return model\n",
    "\n",
    "\n",
    "model=None\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "_ = torch.set_grad_enabled(False)\n",
    "model_path: str = \"meta-llama/Llama-2-13b-chat-hf\"#\"meta-llama/Llama-2-13b-chat-hf\" #even on an A40 I have to load 13b in half precision\n",
    "device: str =  \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "center_weights=False\n",
    "\n",
    "model = load_model(model_path, device, center_weights=center_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"./\"\n",
    "datadir = \"data/\"\n",
    "if \"gpt\" in model_path:\n",
    "    user_tag, asst_tag = \"|<end_of_text>| \", \"\"\n",
    "    model_numlayers = model.config.n_layer\n",
    "else: #/llama/mistral\n",
    "    user_tag, asst_tag = \"[INST] \", \" [/INST]\"\n",
    "    model_numlayers = model.config.num_hidden_layers\n",
    "\n",
    "strip_quotes = lambda s: s[1:-1] if len(s) > 1 and (s.startswith('\"') and s.endswith('\"') or s.startswith(\"'\") and s.endswith(\"'\")) else s\n",
    "\n",
    "pos_label = \"A\"\n",
    "neg_label = \"B\"\n",
    "pos_label_token_id = model.tokenizer.encode(pos_label)[-1]\n",
    "neg_label_token_id = model.tokenizer.encode(neg_label)[-1]\n",
    "df = pd.read_csv(datadir+\"facts_true_false.csv\") #train\n",
    "true_statements = df[df['label'] == 1]['statement']\n",
    "false_statements = df[df['label'] == 0]['statement']\n",
    "\n",
    "true_statements_train = true_statements.sample(n=306, random_state=42)\n",
    "true_statements_test = true_statements.drop(true_statements_train.index)\n",
    "false_statements_train = false_statements.sample(n=306, random_state=42)\n",
    "false_statements_test = false_statements.drop(false_statements_train.index)\n",
    "\n",
    "def template(stmt: str) -> str:\n",
    "    return f\"{user_tag}<<SYS>> Respond only with the letter A or B <<SYS>> Question: Is the following claim true or false? \\\"{strip_quotes(stmt)}\\\"\\n\\nChoices:\\n (A) True\\n (B) False\\n\\nAnswer:{asst_tag} \"\n",
    "\n",
    "train_prompts = []\n",
    "train_labels = []\n",
    "for i, (truth, lie) in enumerate(zip(true_statements_train.values.tolist(), false_statements_train.values.tolist())):\n",
    "    train_prompts.append(template(truth))\n",
    "    train_labels.append(pos_label)\n",
    "    train_prompts.append(template(truth))\n",
    "    train_labels.append(neg_label)\n",
    "    train_prompts.append(template(lie))\n",
    "    train_labels.append(neg_label)\n",
    "    train_prompts.append(template(lie))\n",
    "    train_labels.append(pos_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from typing import List\n",
    "def collate_batch(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]  # Collect attention masks\n",
    "\n",
    "    # Pad input_ids, labels, and attention_masks to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=model.tokenizer.pad_token_id)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Using -100 to ignore padding in loss calculation\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)  # Pad attention masks with zeros\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"attention_mask\": attention_masks_padded\n",
    "    }\n",
    "\n",
    "\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    def __init__(self, prompts: List[str], completions: List[str], tokenizer):\n",
    "        self.prompts = prompts\n",
    "        self.completions = completions\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt_text = self.prompts[idx]\n",
    "        completion_text = self.completions[idx]\n",
    "        \n",
    "        # Tokenize prompt and completion together\n",
    "        encoded_pair = self.tokenizer(prompt_text + completion_text, return_tensors='pt')\n",
    "        input_ids = encoded_pair.input_ids.squeeze(0)\n",
    "        attention_mask = encoded_pair.attention_mask.squeeze(0)  # Create attention mask\n",
    "\n",
    "        # Tokenize completion alone for labels, setting labels for prompt to -100\n",
    "        prompt_ids = self.tokenizer(prompt_text, add_special_tokens=False).input_ids\n",
    "        completion_ids = self.tokenizer(completion_text, add_special_tokens=False).input_ids\n",
    "        labels = [-100] * len(prompt_ids) + completion_ids  # Ignore prompt tokens in loss calculation\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"attention_mask\": attention_mask  # Include attention mask\n",
    "        }\n",
    "        \n",
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = len(data_source) // batch_size\n",
    "        self.batch_indices = list(range(self.num_batches))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.batch_indices) # Shuffle the order of batches\n",
    "        for batch_idx in self.batch_indices:\n",
    "            batch_start = batch_idx * self.batch_size\n",
    "            batch_indices = list(range(batch_start, batch_start + self.batch_size))\n",
    "            random.shuffle(batch_indices) # Shuffle indices within the batch\n",
    "            for idx in batch_indices:\n",
    "                yield idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches * self.batch_size\n",
    "        \n",
    "model.tokenizer.padding_side = \"right\"\n",
    "batch_size=4\n",
    "dataset = PromptCompletionDataset(train_prompts, train_labels, model.tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_batch, sampler=CustomBatchSampler(dataset, batch_size))#, shuffle=False)    \n",
    "#dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_batch, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_train = [14, 15, 16, 17, 18, 19]  \n",
    "layer_prefix = 'transformer.h.' if \"gpt2\" in model_path else 'model.layers.'\n",
    "layernorm_name = '_ln' if \"gpt2\" in model_path else '_layernorm'\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 #for llama\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "#    if (name.startswith(layer_prefix) and int(name.split('.')[2]) not in layers_to_train) or not name.startswith(layer_prefix) or layernorm_name in name or \"mlp\" in name or \"attn.o_\" in name:\n",
    "#        #print(f\"Freezing name={name}, layer={int(name.split('.')[2])}\")\n",
    "#        param.requires_grad = False\n",
    "token_loss_params = []\n",
    "projection_loss_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    need=False\n",
    "    if (name.startswith(layer_prefix) and int(name.split('.')[2]) in layers_to_train) and (\"attn.q_\" in name or \"attn.k_\" in name or \"attn.v_\" in name):\n",
    "        token_loss_params.append(param)\n",
    "        need=True\n",
    "    if (name.startswith(layer_prefix) and int(name.split('.')[2]) in layers_to_train) and (\"attn.q_\" in name or \"attn.k_\" in name or \"attn.v_\" in name):\n",
    "        projection_loss_params.append(param)\n",
    "        need=True\n",
    "    if not need: param.requires_grad = False\n",
    "\n",
    "train_direction_in = True\n",
    "flip_direction = True\n",
    "projection_weight = 1.0\n",
    "num_epochs=3\n",
    "fname = 'directions_llama2_13b_f16_persona_lasttoken_pc2raw.pkl'\n",
    "#fname = 'directions_gpt2xl_f32_persona_lasttoken_pc2raw.pkl'\n",
    "#fname = 'directions_gpt2xl_gpt4facts_persona_lasttoken_pc2raw.pkl'\n",
    "with open(outputdir+fname, 'rb') as f:\n",
    "    directions = pickle.load(f)\n",
    "\n",
    "for layer, tensors in directions.items(): \n",
    "    directions[layer] = [tensor.to(dtype=torch.float16) for tensor in tensors]\n",
    "    for tensor in directions[layer]:\n",
    "        if torch.isnan(tensor).any(): print(f\"NaN in layer {layer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter('runs/experiment_1')\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir runs\n",
    "\n",
    "def print_parameter_stats(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} - norm: {param.norm().item()}\")\n",
    "\n",
    "def print_gradient_stats(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            print(f\"{name} - grad norm: {param.grad.norm().item()}\")\n",
    "\n",
    "def print_gradient_summary_stats(model):\n",
    "    minval = float('inf')\n",
    "    maxval = float('-inf')\n",
    "    cumsum = cumsumsq = cnt = 0\n",
    "    summary_dict = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            cnt += 1\n",
    "            v = param.grad.norm().item()\n",
    "            cumsum += v\n",
    "            cumsumsq += v**2\n",
    "            if v < minval: \n",
    "                minval=v\n",
    "                summary_dict['minval'] = {\"name\": name, \"val\": v}\n",
    "            if v > maxval: \n",
    "                maxval=v\n",
    "                summary_dict['maxval'] = {\"name\": name, \"val\": v}\n",
    "    if cnt == 0: return\n",
    "    summary_dict['mean'] = cumsum/cnt\n",
    "    summary_dict['std'] = (cumsumsq/cnt-summary_dict['mean']**2)**0.5\n",
    "    for k, v in summary_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            print(f\"grad norm: {k} = {v['val']:.6f} at {v['name']}\")\n",
    "        else:\n",
    "            print(f\"grad norm: {k} = {v:.6f}\")\n",
    "            \n",
    "def print_summary_statistics(tensor, name):\n",
    "    if tensor is not None:\n",
    "        print(f\"{name} - min: {tensor.min().item()}, max: {tensor.max().item()}, mean: {tensor.mean().item()}, std: {tensor.std().item()}\")\n",
    "\n",
    "def check_running_averages(optimizer):\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = optimizer.state[p]\n",
    "            if 'exp_avg' in state and 'exp_avg_sq' in state:\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                print_summary_statistics(exp_avg, \"exp_avg\")\n",
    "                print_summary_statistics(exp_avg_sq, \"exp_avg_sq\")\n",
    "\n",
    "def check_parameters(model, message):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print_summary_statistics(param, f\"{message} - {name}\")\n",
    "\n",
    "soft_targets = torch.zeros(batch_size, model.config.vocab_size)\n",
    "for i in range(batch_size):\n",
    "    soft_targets[i, pos_label_token_id] = 0.5\n",
    "    soft_targets[i, neg_label_token_id] = 0.5\n",
    "def soft_target_cross_entropy(logits):\n",
    "    log_softmax_logits = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets.to(log_softmax_logits.device) * log_softmax_logits).sum(dim=1).mean()\n",
    "\n",
    "def focused_prob_loss(logits):#actually this doesn't work: the model quickly learns to put all of its probably mass on one (randomly chosen) of the two tokens, and never varies because the loss is 0\n",
    "    \"\"\"\n",
    "    Custom loss function to penalize the model based on the probability assigned to incorrect tokens.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): The raw output logits from the model (batch_size, vocab_size).\n",
    "    correct_indices (torch.Tensor): A 2D tensor (batch_size, 2) containing indices of the two correct tokens for each example in the batch.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The computed loss.\n",
    "    \"\"\"\n",
    "    # Calculate softmax probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    # Initialize a tensor to gather probabilities of correct tokens\n",
    "    batch_size, vocab_size = logits.shape\n",
    "    correct_probs = torch.zeros(batch_size, device=logits.device)\n",
    "\n",
    "    # Sum probabilities of the two correct tokens\n",
    "    for i in range(batch_size):\n",
    "        correct_probs[i] = probabilities[i, pos_label_token_id] + probabilities[i, neg_label_token_id]\n",
    "\n",
    "    # Compute the loss as the negative log of summed probabilities of correct tokens\n",
    "    # This loss is minimized when correct_probs approaches 1, which happens when\n",
    "    # the model places all its probability mass on the correct tokens.\n",
    "    loss = -torch.log(correct_probs).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "key_token_offset=1\n",
    "priortoks=0\n",
    "torch.manual_seed(123)\n",
    "clear_hooks(model)\n",
    "\n",
    "total_losses = []\n",
    "token_losses = []\n",
    "projection_losses = []\n",
    "num_epochs=1\n",
    "projection_weight=10\n",
    "#train_direction_in=False\n",
    "#flip_direction=False\n",
    "activation_storage = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "learning_rate=5e-5\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': list(set(token_loss_params) - set(projection_loss_params)), 'lr': learning_rate, 'eps': 1e-04, 'weight_decay': 0.01},\n",
    "    {'params': list(set(projection_loss_params) - set(token_loss_params)), 'lr': learning_rate, 'eps': 1e-04, 'weight_decay': 0.01},\n",
    "    {'params': list(set(token_loss_params).intersection(projection_loss_params)), 'lr': learning_rate, 'eps': 1e-04, 'weight_decay': 0.01}\n",
    "])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-04, weight_decay=0.01)\n",
    "###scaler = GradScaler()\n",
    "###lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=10, num_training_steps=(len(dataloader) * num_epochs),)\n",
    "\n",
    "total_steps = (len(dataloader)) * num_epochs\n",
    "warmup_steps = total_steps // 20  \n",
    "\n",
    "def warmup_lambda(step):\n",
    "    return min(1.0, step / warmup_steps)\n",
    "\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=(total_steps - warmup_steps), eta_min=1e-6)\n",
    "\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_token_losses = []\n",
    "    epoch_projection_losses = []\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        #print(\"i=\",i)\n",
    "        optimizer.zero_grad()\n",
    "        last_token_positions = (batch['attention_mask'].sum(dim=1) - key_token_offset).tolist()\n",
    "        layers_positions = {}\n",
    "        for layer in layers_to_train:\n",
    "            layers_positions[layer] = [[pos-i for i in range(priortoks,-1,-1)] for pos in last_token_positions]\n",
    "\n",
    "        activation_storage = defaultdict(lambda: defaultdict(list))\n",
    "###        with autocast(): # for mixed precision training, when you load model in float 16 for memory reasons\n",
    "        attach_activation_hooks(model, layers_positions, activation_storage, \"end\")\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "        #loss = outputs.loss#.to(torch.float16)\n",
    "        loss = soft_target_cross_entropy(logits)\n",
    "        \n",
    "        skip_token_loss = loss.item() < 0.7 * 5 # no need to do this if you're already close to the theoretical min of -ln(0.5); save time and prevent overfitting\n",
    "\n",
    "        if not skip_token_loss: \n",
    "            loss*=0.1\n",
    "            loss.backward(retain_graph=True)\n",
    "            for param in set(projection_loss_params) - set(token_loss_params):\n",
    "                param.grad = None\n",
    "        \n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN or Inf detected in model output (batch {i})\")\n",
    "            raise SystemExit\n",
    "        \n",
    "        cum_projection_loss = 0\n",
    "        for layer, positions in activation_storage.items():\n",
    "            for pos, tensor_list in positions.items():#each of these is a list of batchsize d-embed tensors for a given position\n",
    "                batch_tensor = torch.stack(tensor_list, dim=0)#.float()\n",
    "                if torch.isnan(batch_tensor).any() or torch.isinf(batch_tensor).any():\n",
    "                    print(f\"NaN or Inf detected in batch tensor (layer {layer}, batch {i})\")\n",
    "                    raise SystemExit\n",
    "                direction = (directions[layer][pos] * (-1 if flip_direction else 1)).to(batch_tensor.device)#.to(batch_tensor.dtype)\n",
    "                if torch.isnan(directions[layer][pos]).any() or torch.isinf(directions[layer][pos]).any():\n",
    "                    print(f\"NaN or Inf detected in direction (layer {layer}, batch {i})\")\n",
    "                    raise SystemExit\n",
    "                projection = (batch_tensor @ direction) / (torch.norm(direction) * torch.norm(batch_tensor, dim=1))\n",
    "                if torch.isnan(projection).any(): \n",
    "                    print(f\"NaN in layer {layer}\")\n",
    "                    raise SystemExit\n",
    "                #else: print(\"All good\")\n",
    "                if train_direction_in: projection_loss = ((1 - projection) / 2) * .1# ranges from 0 for perfect alignment to 1 for perfect anti-alignment\n",
    "#                projection = (batch_tensor @ direction.to(batch_tensor.dtype)) / (torch.norm(direction))\n",
    "#                if train_direction_in: projection_loss = (1 - torch.tanh(projection*0.01) / len(layers_to_train)) / 2#1/projection#(1 - projection / len(layers_to_train)) / 2 # ranges from 0 for perfect alignment to 1 for perfect anti-alignment\n",
    "                else: projection_loss = torch.abs(projection) # 0 if no projection, 1 if perfect anti-alignment\n",
    "                cum_projection_loss += projection_loss.mean() / (len(layers_to_train) * len(positions.items())) #average over batch, as with token loss\n",
    "        \n",
    "        cum_projection_loss.backward()\n",
    "        if not skip_token_loss:\n",
    "            for param in set(token_loss_params) - set(projection_loss_params):\n",
    "                param.grad = None\n",
    "        optimizer.step()\n",
    "        ######total_loss = loss + cum_projection_loss * projection_weight\n",
    "\n",
    "#        total_loss = total_loss.to(torch.float16)\n",
    "        ######total_loss.backward()\n",
    "###        scaler.scale(total_loss).backward()\n",
    "###        scaler.unscale_(optimizer)\n",
    "#        check_parameters(model,\"pre\")\n",
    "        if (i+1)%40 == 0:\n",
    "            print_gradient_summary_stats(model)\n",
    "        # Gradient clipping\n",
    "######        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#        check_running_averages(optimizer)\n",
    "###        optimizer.step()\n",
    "#        check_running_averages(optimizer,)\n",
    "#        check_parameters(model,\"post\")\n",
    "#        for name, param in model.named_parameters():\n",
    "#            if param.grad is not None:\n",
    "#                if param.grad.dtype == torch.float16:\n",
    "#                    param.grad.data = param.grad.data.to(torch.float32)\n",
    "\n",
    "#        print(\"Gradient before unscaling:\", model.transformer.h[14].ln_1.weight.grad)  \n",
    "#        print(\"Gradient after unscaling:\", model.transformer.h[14].ln_1.weight.grad)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradient of {name} after unscaling\")\n",
    "                raise SystemExit  # Or handle the error gracefully\n",
    "\n",
    "#        for name, param in model.named_parameters():\n",
    "#            if param.grad is not None:\n",
    "#                if param.grad.dtype == torch.float32:\n",
    "#                    param.grad.data = param.grad.data.to(torch.float16)\n",
    "###        scaler.step(optimizer)\n",
    "###        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        for block in get_blocks(model): block._forward_hooks.clear()\n",
    "        activation_storage.clear()\n",
    "\n",
    "        epoch_token_losses.append(loss.item())\n",
    "        epoch_projection_losses.append(cum_projection_loss.item())\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f\"Prediction: {model.tokenizer.convert_ids_to_tokens(predicted_labels.tolist())}\")\n",
    "            print(f\"Token Loss: {loss.item():.4f}, Projection loss: {cum_projection_loss.item():.4f}\")\n",
    "            #for name, param in model.named_parameters():\n",
    "            #   if param.grad is not None:\n",
    "            #       print(name, param.grad.norm().item())\n",
    "                    #writer.add_scalar(f'Gradient/{name}', param.grad.norm().item(), epoch)\n",
    "    token_losses.append(sum(epoch_token_losses) / len(epoch_token_losses))\n",
    "    projection_losses.append(sum(epoch_projection_losses) / len(epoch_projection_losses))\n",
    "    print(f\"Avg Token Prediction Loss: {token_losses[epoch]:.4f}\")\n",
    "    print(f\"Avg Projection Loss: {projection_losses[epoch]:.4f}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(token_losses, label='Token Prediction Loss')\n",
    "plt.plot(projection_losses, label='Projection Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Trends over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"pansamuel/llama_13b_honest_tune_in\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
