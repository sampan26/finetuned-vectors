{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/cma1114/enhanced_hooking.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers matplotlib pandas scikit-learn seaborn datasets\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from enhanced_hooking import get_blocks, clear_hooks, attach_activation_hooks, attach_zerograd_hooks, remove_zerograd_hooks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import gc\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import Sampler\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN='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'\n",
    "def load_model(model_path, device, center_weights=True):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, token=HF_TOKEN).to(device)\n",
    "    if center_weights:\n",
    "        for name, param in model.named_parameters():\n",
    "            if '.'.join(name.split('.')[-2:]) in ['wte.weight','wpe.weight','c_proj.weight','c_proj.bias']:\n",
    "                param.data -= param.data.mean()\n",
    "                print(name, param.data.mean(), param.size())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=HF_TOKEN)\n",
    "    model.tokenizer = tokenizer\n",
    "    model.tokenizer.pad_token_id = model.tokenizer.eos_token_id \n",
    "    return model\n",
    "\n",
    "model=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_path: str = \"meta-llama/Llama-2-13b-chat-hf\" #even on an A40 I have to load 13b in half precision\n",
    "device: str = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "center_weights=False\n",
    "\n",
    "model = load_model(model_path, device, center_weights=center_weights)\n",
    "print(model.get_memory_footprint())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"output/\"\n",
    "datadir = \"data/\"\n",
    "if \"gpt\" in model_path:\n",
    "    user_tag, asst_tag = \"|<end_of_text>| \", \"\"\n",
    "    model_numlayers = model.config.n_layer\n",
    "else: #/llama/mistral\n",
    "    user_tag, asst_tag = \"[INST] \", \" [/INST]\"\n",
    "    model_numlayers = model.config.num_hidden_layers\n",
    "\n",
    "strip_quotes = lambda s: s[1:-1] if len(s) > 1 and (s.startswith('\"') and s.endswith('\"') or s.startswith(\"'\") and s.endswith(\"'\")) else s\n",
    "\n",
    "pos_label = \"A\"\n",
    "neg_label = \"B\"\n",
    "pos_label_token_id = model.tokenizer.encode(pos_label)[-1]\n",
    "neg_label_token_id = model.tokenizer.encode(neg_label)[-1]\n",
    "df = pd.read_csv(datadir+\"facts_true_false.csv\") #train\n",
    "true_statements = df[df['label'] == 1]['statement']\n",
    "false_statements = df[df['label'] == 0]['statement']\n",
    "\n",
    "true_statements_train = true_statements.sample(n=306, random_state=42)\n",
    "true_statements_test = true_statements.drop(true_statements_train.index)\n",
    "false_statements_train = false_statements.sample(n=306, random_state=42)\n",
    "false_statements_test = false_statements.drop(false_statements_train.index)\n",
    "\n",
    "def template(stmt: str) -> str:\n",
    "    return f\"{user_tag}<<SYS>> Respond only with the letter A or B <<SYS>> Question: Is the following claim true or false? \\\"{strip_quotes(stmt)}\\\"\\n\\nChoices:\\n (A) True\\n (B) False\\n\\nAnswer:{asst_tag} \"\n",
    "\n",
    "train_prompts = []\n",
    "train_labels = []\n",
    "for i, (truth, lie) in enumerate(zip(true_statements_train.values.tolist(), false_statements_train.values.tolist())):\n",
    "    train_prompts.append(template(truth))\n",
    "    train_labels.append(pos_label)\n",
    "    train_prompts.append(template(truth))\n",
    "    train_labels.append(neg_label)\n",
    "    train_prompts.append(template(lie))\n",
    "    train_labels.append(neg_label)\n",
    "    train_prompts.append(template(lie))\n",
    "    train_labels.append(pos_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]  # Collect attention masks\n",
    "\n",
    "    # Pad input_ids, labels, and attention_masks to the maximum length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=model.tokenizer.pad_token_id)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Using -100 to ignore padding in loss calculation\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)  # Pad attention masks with zeros\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"attention_mask\": attention_masks_padded\n",
    "    }\n",
    "\n",
    "\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    def __init__(self, prompts: List[str], completions: List[str], tokenizer):\n",
    "        self.prompts = prompts\n",
    "        self.completions = completions\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt_text = self.prompts[idx]\n",
    "        completion_text = self.completions[idx]\n",
    "        \n",
    "        # Tokenize prompt and completion together\n",
    "        encoded_pair = self.tokenizer(prompt_text + completion_text, return_tensors='pt')\n",
    "        input_ids = encoded_pair.input_ids.squeeze(0)\n",
    "        attention_mask = encoded_pair.attention_mask.squeeze(0)  # Create attention mask\n",
    "\n",
    "        # Tokenize completion alone for labels, setting labels for prompt to -100\n",
    "        prompt_ids = self.tokenizer(prompt_text, add_special_tokens=False).input_ids\n",
    "        completion_ids = self.tokenizer(completion_text, add_special_tokens=False).input_ids\n",
    "        labels = [-100] * len(prompt_ids) + completion_ids  # Ignore prompt tokens in loss calculation\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"attention_mask\": attention_mask  # Include attention mask\n",
    "        }\n",
    "        \n",
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = len(data_source) // batch_size\n",
    "        self.batch_indices = list(range(self.num_batches))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.batch_indices) # Shuffle the order of batches\n",
    "        for batch_idx in self.batch_indices:\n",
    "            batch_start = batch_idx * self.batch_size\n",
    "            batch_indices = list(range(batch_start, batch_start + self.batch_size))\n",
    "            random.shuffle(batch_indices) # Shuffle indices within the batch\n",
    "            for idx in batch_indices:\n",
    "                yield idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches * self.batch_size\n",
    "        \n",
    "model.tokenizer.padding_side = \"right\"\n",
    "batch_size=4\n",
    "dataset = PromptCompletionDataset(train_prompts, train_labels, model.tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_batch, sampler=CustomBatchSampler(dataset, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_train = [14, 15, 16, 17, 18, 19]  \n",
    "layer_prefix = 'transformer.h.' if \"gpt2\" in model_path else 'model.layers.'\n",
    "layernorm_name = '_ln' if \"gpt2\" in model_path else '_layernorm'\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 #for llama\n",
    "\n",
    "token_loss_params = []\n",
    "projection_loss_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    need=False\n",
    "    if (name.startswith(layer_prefix) and int(name.split('.')[2]) in layers_to_train) and (\"mlp.gate_proj\" in name):\n",
    "        token_loss_params.append(param)\n",
    "        need=True\n",
    "    if (name.startswith(layer_prefix) and int(name.split('.')[2]) in layers_to_train) and (\"attn.v_\" in name or \"attn.o_\" in name):\n",
    "        projection_loss_params.append(param)\n",
    "        need=True\n",
    "    if not need: param.requires_grad = False\n",
    "\n",
    "fname = 'directions_llama2_13b_f16_persona_lasttoken_pc2raw.pkl'\n",
    "\n",
    "with open(outputdir+fname, 'rb') as f:\n",
    "    directions = pickle.load(f)\n",
    "\n",
    "for layer, tensors in directions.items(): \n",
    "    directions[layer] = [tensor.to(dtype=torch.float16) for tensor in tensors]\n",
    "    for tensor in directions[layer]:\n",
    "        if torch.isnan(tensor).any(): print(f\"NaN in layer {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "# Functions to monitor gradients and parameters\n",
    "def print_parameter_stats(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} - norm: {param.norm().item()}\")\n",
    "\n",
    "def print_gradient_stats(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            print(f\"{name} - grad norm: {param.grad.norm().item()}\")\n",
    "\n",
    "def print_gradient_summary_stats(model):\n",
    "    minval = float('inf')\n",
    "    maxval = float('-inf')\n",
    "    cumsum = cumsumsq = cnt = 0\n",
    "    summary_dict = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            cnt += 1\n",
    "            v = param.grad.norm().item()\n",
    "            cumsum += v\n",
    "            cumsumsq += v**2\n",
    "            if v < minval: \n",
    "                minval=v\n",
    "                summary_dict['minval'] = {\"name\": name, \"val\": v}\n",
    "            if v > maxval: \n",
    "                maxval=v\n",
    "                summary_dict['maxval'] = {\"name\": name, \"val\": v}\n",
    "    if cnt == 0: return\n",
    "    summary_dict['mean'] = cumsum/cnt\n",
    "    summary_dict['std'] = (cumsumsq/cnt-summary_dict['mean']**2)**0.5\n",
    "    for k, v in summary_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            print(f\"grad norm: {k} = {v['val']:.6f} at {v['name']}\")\n",
    "        else:\n",
    "            print(f\"grad norm: {k} = {v:.6f}\")\n",
    "            \n",
    "def print_summary_statistics(tensor, name):\n",
    "    if tensor is not None:\n",
    "        print(f\"{name} - min: {tensor.min().item()}, max: {tensor.max().item()}, mean: {tensor.mean().item()}, std: {tensor.std().item()}\")\n",
    "\n",
    "def check_running_averages(optimizer):\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = optimizer.state[p]\n",
    "            if 'exp_avg' in state and 'exp_avg_sq' in state:\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                print_summary_statistics(exp_avg, \"exp_avg\")\n",
    "                print_summary_statistics(exp_avg_sq, \"exp_avg_sq\")\n",
    "\n",
    "def check_parameters(model, message):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print_summary_statistics(param, f\"{message} - {name}\")\n",
    "\n",
    "def focused_prob_loss(logits):#actually this doesn't work: the model quickly learns to put all of its probably mass on one (randomly chosen) of the two tokens, and never varies because the loss is 0\n",
    "    \"\"\"\n",
    "    Custom loss function to penalize the model based on the probability assigned to incorrect tokens.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): The raw output logits from the model (batch_size, vocab_size).\n",
    "    correct_indices (torch.Tensor): A 2D tensor (batch_size, 2) containing indices of the two correct tokens for each example in the batch.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The computed loss.\n",
    "    \"\"\"\n",
    "    # Calculate softmax probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    # Initialize a tensor to gather probabilities of correct tokens\n",
    "    batch_size, vocab_size = logits.shape\n",
    "    correct_probs = torch.zeros(batch_size, device=logits.device)\n",
    "\n",
    "    # Sum probabilities of the two correct tokens\n",
    "    for i in range(batch_size):\n",
    "        correct_probs[i] = probabilities[i, pos_label_token_id] + probabilities[i, neg_label_token_id]\n",
    "\n",
    "    # Compute the loss as the negative log of summed probabilities of correct tokens\n",
    "    # This loss is minimized when correct_probs approaches 1, which happens when\n",
    "    # the model places all its probability mass on the correct tokens.\n",
    "    loss = -torch.log(correct_probs).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "#### Token loss function: target dist is 50% A, 50% B, 0% any other token\n",
    "soft_targets = torch.zeros(batch_size, model.config.vocab_size)\n",
    "for i in range(batch_size):\n",
    "    soft_targets[i, pos_label_token_id] = 0.5\n",
    "    soft_targets[i, neg_label_token_id] = 0.5\n",
    "def soft_target_cross_entropy(logits):\n",
    "    log_softmax_logits = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets.to(log_softmax_logits.device) * log_softmax_logits).sum(dim=1).mean()\n",
    "##################################################\n",
    "\n",
    "key_token_offset=1\n",
    "priortoks=0\n",
    "num_epochs=2\n",
    "projection_weight=10\n",
    "train_direction_in = True # train behavior vector in or out\n",
    "flip_direction = False # train pos or neg behavior\n",
    "max_projection_in = 0.5 # how far \"in\" to tune the projection (max 1); too far can degrade output \n",
    "\n",
    "torch.manual_seed(123)\n",
    "learning_rate_tok=1e-5\n",
    "learning_rate_proj=5e-5\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': list(set(token_loss_params) - set(projection_loss_params)), 'lr': learning_rate_tok, 'eps': 1e-04, 'weight_decay': 0.01},\n",
    "    {'params': list(set(projection_loss_params) - set(token_loss_params)), 'lr': learning_rate_proj, 'eps': 1e-04, 'weight_decay': 0.01},\n",
    "    {'params': list(set(token_loss_params).intersection(projection_loss_params)), 'lr': learning_rate_tok, 'eps': 1e-04, 'weight_decay': 0.01}\n",
    "])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-04, weight_decay=0.01)\n",
    "\n",
    "total_steps = (len(dataloader)) * num_epochs\n",
    "warmup_steps = total_steps // 20  \n",
    "\n",
    "def warmup_lambda(step):\n",
    "    return min(1.0, step / warmup_steps)\n",
    "\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=(total_steps - warmup_steps), eta_min=1e-6)\n",
    "\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=10, num_training_steps=(len(dataloader) * num_epochs),)\n",
    "\n",
    "total_token_losses = []\n",
    "total_projection_losses = []\n",
    "batch_token_losses = []\n",
    "batch_projection_losses = []\n",
    "activation_storage = defaultdict(lambda: defaultdict(list))\n",
    "clear_hooks(model)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_token_losses = []\n",
    "    epoch_projection_losses = []\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        last_token_positions = (batch['attention_mask'].sum(dim=1) - key_token_offset).tolist()\n",
    "        layers_positions = {}\n",
    "        for layer in layers_to_train:\n",
    "            layers_positions[layer] = [[pos-i for i in range(priortoks,-1,-1)] for pos in last_token_positions]\n",
    "\n",
    "        activation_storage = defaultdict(lambda: defaultdict(list))\n",
    "        attach_activation_hooks(model, layers_positions, activation_storage, \"end\")\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, -1, :]  # Logits of last output token\n",
    "        ###loss = outputs.loss#.to(torch.float16)\n",
    "        loss = soft_target_cross_entropy(logits)\n",
    "        \n",
    "        skip_token_loss = loss.item() < 0.7 # no need to do this if you're already close to the theoretical min of -ln(0.5); save time and prevent overfitting\n",
    "\n",
    "        if not skip_token_loss: \n",
    "            #loss*=0.25\n",
    "            # Disable gradients for projection_loss_params\n",
    "            projection_handles = attach_zerograd_hooks(set(projection_loss_params) - set(token_loss_params))\n",
    "            loss.backward(retain_graph=True)\n",
    "            remove_zerograd_hooks(projection_handles)\n",
    "        \n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN or Inf detected in model output (batch {i})\")\n",
    "            raise SystemExit\n",
    "        \n",
    "        cum_projection_loss = 0\n",
    "        for layer, positions in activation_storage.items():\n",
    "            for pos, tensor_list in positions.items():#each of these is a list of batchsize d-embed tensors for a given position\n",
    "                batch_tensor = torch.stack(tensor_list, dim=0)\n",
    "                direction = (directions[layer][pos] * (-1 if flip_direction else 1)).to(batch_tensor.device)#.to(batch_tensor.dtype)\n",
    "                projection = (batch_tensor @ direction) / (torch.norm(direction) * torch.norm(batch_tensor, dim=1))\n",
    "                if train_direction_in: projection_loss = ((1 - projection) / 2) * max_projection_in # ranges from 0 for perfect alignment to max_projection_in for perfect anti-alignment\n",
    "                else: projection_loss = torch.abs(projection) # 0 if no projection, 1 if perfect anti-alignment\n",
    "                cum_projection_loss += projection_loss.mean() / (len(layers_to_train) * len(positions.items())) #average over batch, as with token loss\n",
    "        \n",
    "        if not skip_token_loss: token_handles = attach_zerograd_hooks(set(token_loss_params) - set(projection_loss_params)) # Don't propagate token loss\n",
    "        cum_projection_loss.backward()\n",
    "        if not skip_token_loss: remove_zerograd_hooks(token_handles)\n",
    "        optimizer.step()\n",
    "\n",
    "        \"\"\"          ### separate losses are better/faster\n",
    "        total_loss = loss + cum_projection_loss * projection_weight  \n",
    "        total_loss.backward()\n",
    "#        check_parameters(model,\"pre\")\n",
    "        # Gradient clipping\n",
    "######        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#        check_running_averages(optimizer)\n",
    "        optimizer.step()\n",
    "#        check_running_averages(optimizer,)\n",
    "#        check_parameters(model,\"post\")\n",
    "        \"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in gradient of {name} after unscaling\")\n",
    "                raise SystemExit  # Or handle the error gracefully\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if (i+1)%40 == 0:\n",
    "            print_gradient_summary_stats(model)\n",
    "        for block in get_blocks(model): block._forward_hooks.clear()\n",
    "        activation_storage.clear()\n",
    "\n",
    "        epoch_token_losses.append(loss.item())\n",
    "        epoch_projection_losses.append(cum_projection_loss.item())\n",
    "        batch_token_losses.append(loss.item())\n",
    "        batch_projection_losses.append(cum_projection_loss.item())\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f\"Prediction: {model.tokenizer.convert_ids_to_tokens(predicted_labels.tolist())}\")\n",
    "            print(f\"Token Loss: {loss.item():.4f}, Projection loss: {cum_projection_loss.item():.4f}\")\n",
    "    total_token_losses.append(sum(epoch_token_losses) / len(epoch_token_losses))\n",
    "    total_projection_losses.append(sum(epoch_projection_losses) / len(epoch_projection_losses))\n",
    "    print(f\"Avg Token Prediction Loss: {total_token_losses[epoch]:.4f}\")\n",
    "    print(f\"Avg Projection Loss: {total_projection_losses[epoch]:.4f}\")\n",
    "\n",
    "# Plot losses (on separate axes)\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax1 = plt.gca()  \n",
    "ax1.plot(batch_token_losses, label='Token Prediction Loss', color='b')  \n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Token Loss', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_title('Loss Trends over Epochs')\n",
    "ax2 = ax1.twinx()  \n",
    "ax2.plot(batch_projection_losses, label='Projection Loss', color='r') \n",
    "ax2.set_ylabel('Projection Loss', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"pansamuel/llama_13b_honest_tune_in\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
