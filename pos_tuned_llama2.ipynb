{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers matplotlib pandas scikit-learn seaborn datasets\n",
    "\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "# from enhanced_hooking import get_activations, add_activations_and_generate, clear_hooks, get_activations_and_generate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN='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'\n",
    "def load_model(model_path, device, center_weights=True):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, token=HF_TOKEN).to(device)\n",
    "    if center_weights:\n",
    "        for name, param in model.named_parameters():\n",
    "            if '.'.join(name.split('.')[-2:]) in ['wte.weight','wpe.weight','c_proj.weight','c_proj.bias']:\n",
    "                param.data -= param.data.mean()\n",
    "                print(name, param.data.mean(), param.size())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", token=HF_TOKEN)\n",
    "    model.tokenizer = tokenizer\n",
    "    model.tokenizer.padding_side = \"left\" #for batching; right (default in gpt2) for training, left for generation\n",
    "    model.tokenizer.pad_token_id = model.tokenizer.eos_token_id \n",
    "    return model\n",
    "\n",
    "\n",
    "model=None\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "_ = torch.set_grad_enabled(False)\n",
    "model_path: str = \"pansamuel/llama-2-honesty-tune-in-pos\"#\"meta-llama/Llama-2-13b-chat-hf\" #even on an A40 I have to load 13b in half precision\n",
    "device: str =  \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "center_weights=False\n",
    "\n",
    "model = load_model(model_path, device, center_weights=center_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"./\"\n",
    "datadir = \"data/\"\n",
    "user_tag, asst_tag = \"[INST] \", \" [/INST]\"\n",
    "sys_tag_open, sys_tag_closed = \"<<SYS>> \", \" <</SYS>>\"\n",
    "model_numlayers = model.config.num_hidden_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datadir+\"gpt4_facts.csv\") #eval\n",
    "\n",
    "def template_test(suffix: str) -> str:\n",
    "    return f\"{user_tag}Question: Is the following claim true or false? {suffix}:{asst_tag}\"\n",
    "\n",
    "prompts_to_be_steered = []\n",
    "for statement in df['statement']:\n",
    "    prompts_to_be_steered.append(template_test(statement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "question = prompts_to_be_steered[0]\n",
    "inputs = model.tokenizer(question, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "generated_tokens = model.generate(**inputs, **sampling_kwargs)\n",
    "\n",
    "decoded_output = model.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
