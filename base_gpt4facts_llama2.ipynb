{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/cma1114/enhanced_hooking.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers matplotlib pandas scikit-learn seaborn datasets\n",
    "\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "# from enhanced_hooking import get_activations, add_activations_and_generate, clear_hooks, get_activations_and_generate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN='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'\n",
    "def load_model(model_path, device, center_weights=True):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, token=HF_TOKEN).to(device)\n",
    "    if center_weights:\n",
    "        for name, param in model.named_parameters():\n",
    "            if '.'.join(name.split('.')[-2:]) in ['wte.weight','wpe.weight','c_proj.weight','c_proj.bias']:\n",
    "                param.data -= param.data.mean()\n",
    "                print(name, param.data.mean(), param.size())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=HF_TOKEN)\n",
    "    model.tokenizer = tokenizer\n",
    "    model.tokenizer.padding_side = \"left\" #for batching; right (default in gpt2) for training, left for generation\n",
    "    model.tokenizer.pad_token_id = model.tokenizer.eos_token_id \n",
    "    return model\n",
    "\n",
    "\n",
    "model=None\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "_ = torch.set_grad_enabled(False)\n",
    "model_path: str = \"meta-llama/Llama-2-13b-chat-hf\"#\"meta-llama/Llama-2-13b-chat-hf\" #even on an A40 I have to load 13b in half precision\n",
    "device: str =  \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "center_weights=False\n",
    "\n",
    "model = load_model(model_path, device, center_weights=center_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"data/\"\n",
    "user_tag, asst_tag = \"[INST] \", \" [/INST]\"\n",
    "sys_tag_open, sys_tag_closed = \"<<SYS>> \", \" <</SYS>>\"\n",
    "model_numlayers = model.config.num_hidden_layers\n",
    "\n",
    "strip_quotes = lambda s: s[1:-1] if len(s) > 1 and (s.startswith('\"') and s.endswith('\"') or s.startswith(\"'\") and s.endswith(\"'\")) else s\n",
    "batch_size = 32\n",
    "pos_persona = \"Pretend that you are always honest, no matter what.\"\n",
    "neg_persona = \"Pretend that you are dishonest when it serves you.\"\n",
    "pos_label = \"(A)\"\n",
    "neg_label = \"(B)\"\n",
    "key_token_offset=1\n",
    "pair_statements = False\n",
    "\n",
    "# def template(persona: str, stmt: str, label: str) -> str:\n",
    "#     return f\"{user_tag}{sys_tag_open}{persona}{sys_tag_closed} Question: Is the following claim true or false? \\\"{strip_quotes(stmt)}\\\"\\n\\nChoices:\\n (A) True\\n (B) False\\n\\nAnswer:{asst_tag} {label}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datadir+\"gpt4_facts.csv\") #eval\n",
    "\n",
    "def template_test(suffix: str) -> str:\n",
    "    return f\"{user_tag}Question: Is the following claim true or false? {suffix}\\n\\nChoices:\\n (A) True\\n (B) False\\n\\nAnswer:{asst_tag}\"\n",
    "\n",
    "prompts_to_be_steered = []\n",
    "for statement in df['statement']:\n",
    "    prompts_to_be_steered.append(template_test(statement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from enhanced_hooking import zeroout_projections_and_generate\n",
    "# sampling_kwargs = {\"use_cache\": True#false\n",
    "#                    ,\"pad_token_id\": model.tokenizer.eos_token_id, \"max_new_tokens\": 30, \"do_sample\": False, \"repetition_penalty\": 1.1}\n",
    "sampling_kwargs = {\"use_cache\": True, \"pad_token_id\": model.tokenizer.eos_token_id,\n",
    "                    \"max_new_tokens\": 60\n",
    "                    , \"temperature\": 0.5\n",
    "                   , \"top_p\": 0.3\n",
    "                  , \"do_sample\": False #True\n",
    "                  , \"repetition_penalty\": 1.1 #2.0\n",
    "                   ,\"penalty_alpha\": 0.6 \n",
    "                   ,\"top_k\": 4\n",
    "                    }\n",
    "fname = \"base_gpt4facts_llama2-13b-freaky\"\n",
    "main_file_path = outputdir+fname+\".json\"\n",
    "tmp_file_path = outputdir+fname+\"_tmp.json\"\n",
    "\n",
    "batched_inputs = [\n",
    "    prompts_to_be_steered[p:p+batch_size] for p in range(0, len(prompts_to_be_steered), batch_size)\n",
    "]\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_batch_decode(generated_tokens, input_ids, tokenizer):\n",
    "    batch_size = generated_tokens.shape[0]\n",
    "    start_indices = input_ids.shape[1]\n",
    "    max_len = generated_tokens.shape[1] - start_indices\n",
    "    tokens_to_decode = torch.full((batch_size, max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        len_to_decode = generated_tokens.shape[1] - input_ids.shape[1]\n",
    "        tokens_to_decode[i, :len_to_decode] = generated_tokens[i, input_ids.shape[1]:]\n",
    "    return tokenizer.batch_decode(tokens_to_decode, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_inputs:\n",
    "    model.to(device)\n",
    "    inputs = model.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k,v in inputs.items()}\n",
    "    generated_tokens = model.generate(**inputs, **sampling_kwargs)\n",
    "    original_output = do_batch_decode(generated_tokens, inputs['input_ids'], model.tokenizer)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        current_prompt = batch[i]\n",
    "        current_original_output = original_output[i]\n",
    "        \n",
    "        results.append({\n",
    "            \"sentence\": current_prompt,\n",
    "            \"answer_neut\": current_original_output,\n",
    "        }) \n",
    "    \n",
    "    print(f\"Finished sentence {len(results)}\")\n",
    "\n",
    "    try:\n",
    "        with open(tmp_file_path, \"w\") as rfile:\n",
    "            json.dump(results, rfile)\n",
    "        os.replace(tmp_file_path, main_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write data: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
